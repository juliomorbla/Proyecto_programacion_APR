{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "*   Alumno 1: Julio Moreno Blanco\n",
        "*   Alumno 2: Mireia Carbó Feliu\n",
        "*   Alumno 3: Bryan Duque Gutiérrez\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "S_YDFwZ-JscI"
      },
      "outputs": [],
      "source": [
        "# # ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "# mount='/content/gdrive'\n",
        "# drive_root = mount + \"/My Drive/08_MIAR/Proyecto_programacion_APR\"\n",
        "\n",
        "# try:\n",
        "#   from google.colab import drive\n",
        "#   IN_COLAB=True\n",
        "# except:\n",
        "#   IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6n7MIefJ21i",
        "outputId": "09923d62-90ca-4ed7-9bba-2e1342217886"
      },
      "outputs": [],
      "source": [
        "# # Switch to the directory on the Google Drive that you want to use\n",
        "# import os\n",
        "# if IN_COLAB:\n",
        "#   print(\"We're running Colab\")\n",
        "\n",
        "#   if IN_COLAB:\n",
        "#     # Mount the Google Drive at mount\n",
        "#     print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "#     drive.mount(mount)\n",
        "\n",
        "#     # Create drive_root if it doesn't exist\n",
        "#     create_drive_root = True\n",
        "#     if create_drive_root:\n",
        "#       print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "#       os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "#     # Change to the directory\n",
        "#     print(\"\\nColab: Changing directory to \", drive_root)\n",
        "#     %cd $drive_root\n",
        "# # Verify we're in the correct working directory\n",
        "# %pwd\n",
        "# print(\"Archivos en el directorio: \")\n",
        "# print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iV5tYdRtpik-"
      },
      "outputs": [],
      "source": [
        "# Desinstalar versiones incompatibles preinstaladas en Colab\n",
        "#!pip uninstall -y gym keras keras-nightly keras-Preprocessing keras-vis keras-applications tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_JHXdjwzqVfP",
        "outputId": "7f70a4bc-1f48-42ce-9159-92fb42e08274"
      },
      "outputs": [],
      "source": [
        "# if IN_COLAB:\n",
        "#   %pip install gym==0.17.3\n",
        "#   %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "#   %pip install keras-rl2==1.0.5\n",
        "#   %pip install tensorflow==2.12.0\n",
        "\n",
        "# else:\n",
        "#   %pip install gym==0.17.3\n",
        "#   %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "#   %pip install pyglet==1.5.0\n",
        "#   %pip install h5py==3.1.0\n",
        "#   %pip install Pillow==9.5.0\n",
        "#   %pip install keras-rl2==1.0.5\n",
        "#   %pip install Keras==2.2.4\n",
        "#   %pip install tensorflow==2.5.3\n",
        "#   %pip install torch==2.0.1\n",
        "#   %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyvAmFFK-uHY"
      },
      "source": [
        "### **Introducción**\n",
        "\n",
        "El entorno sobre el que vamos a trabajar es SpaceInvaders-v0, que pertenece a la colección de Atari Games disponible en Gymnasium. Es una versión del videojuego Space Invaders, donde controlasuna nave que se mueve lateralmente por la parte inferior de la pantalla y dispara a los enemigos que van bajando de la parte superior de la pantalla.\n",
        "\n",
        "El **objetivo principal** es sobrevivir y destruir enemigos, y se obtienen recomensas cuando se destruye al enemigo.\n",
        "\n",
        "Antes de implementar los modelos, debemos tener en cuenta diferentes cosas:\n",
        "\n",
        "\n",
        "*   Las observaciones son de tipo imagen RGB de 210x160 píxeles con 3 canales. Por lo tanto, nuestro **input** es de tipo imagen.\n",
        "*   Se pueden realizar diferentes tipos de **acciones**:\n",
        "    * 0: NOOP (no hacer nada)\n",
        "    * 1: FIRE (dispara)\n",
        "    * 2: RIGHT\n",
        "    * 3: LEFT\n",
        "    * 4: RIGHTFIRE\n",
        "    * 5: LEFTFIRE\n",
        "* **Recompensas**: Cada enemigo destruido da una recompensa positiva. Si pierdes una vida, no necesariamente recibes recompensa negativa, pero el episodio puede terminar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3eRhgI-Gb2a"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Dense, Activation, Flatten, Convolution2D, Permute, Input,\n",
        "                                     SeparableConv2D, Lambda, Subtract, Add, Conv2D,\n",
        "                                     DepthwiseConv2D, BatchNormalization, ReLU, InputLayer)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import time\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from rl.callbacks import FileLogger\n",
        "\n",
        "from gym import wrappers\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "### **Configuración base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jwOE6I_KGb2a"
      },
      "outputs": [],
      "source": [
        "# Parámetros\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "# Entorno\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9jGEZUcpGb2a"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMglriV87VOa",
        "outputId": "7c24c46b-8611-475f-c03d-f9da4adbb0a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numero de acciones disponibles: 6\n"
          ]
        }
      ],
      "source": [
        "print(\"Numero de acciones disponibles: \" + str(nb_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvfPBYNU7cKH",
        "outputId": "e9570af2-d9a5-4295-fa13-ec9809ae3dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formato de las observaciones:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Formato de las observaciones:\")\n",
        "env.observation_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb3_y2okCbz1"
      },
      "source": [
        "Tal y como hemos visto, vamos a trabajar con las 6 acciones mencionadas anteriormente y con imágenes RGB de 210x160 píxeles y valores de intensidad de 0 a 255, lo cual es una entrada de alta dimensionalidad y con información que, en nuestro caso, puede ser rebundante o irrelevante para la toma de decisiones.\n",
        "\n",
        "Es por ello que creemos que puede ser mejor aplicar un preprocesamiento de la imagen, a una conversion de tonos grises, y reescalando la imagen.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "88RImiauB7pV",
        "outputId": "a25ba917-015d-4858-d3a0-cc1e404f3e9b"
      },
      "outputs": [],
      "source": [
        "# # Obtener un estado inicial del entorno\n",
        "# obs = env.reset()\n",
        "# processor = AtariProcessor()\n",
        "\n",
        "# # Procesar la observación\n",
        "# processed_obs = processor.process_observation(obs)\n",
        "\n",
        "# # Mostrar imagen procesada\n",
        "# plt.imshow(processed_obs, cmap='gray')\n",
        "# plt.title(\"Observación procesada:\")\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "## **1. Implementación de la red neuronal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cakby6-3rEf"
      },
      "source": [
        "Los hiperparámetros de la red neuronal aplicados son:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yoTHazNuOEPy"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.00025\n",
        "GAMMA = 0.99\n",
        "TARGET_MODEL_UPDATE = 10000\n",
        "DELTA_CLIP = 1.0\n",
        "TRAIN_INTERVAL = 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNT-oRDeOIq1"
      },
      "source": [
        "### MODELO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TZjKw0B0OmQ9"
      },
      "outputs": [],
      "source": [
        "def create_mobile_like_dqn(input_shape, num_actions):\n",
        "    x_in = Input(shape=(input_shape[0], input_shape[1], input_shape[2]))\n",
        "    x = Permute((2, 3, 1))(x_in)  # (84,84,frames)\n",
        "    # Primer conv estándar ligero\n",
        "    x = Conv2D(16, (3, 3), strides=2, padding='same')(x)  # ~(42x42x16)\n",
        "    x = BatchNormalization()(x); x = ReLU()(x)\n",
        "    # Bloque depthwise-pointwise\n",
        "    def dw_pw(block_input, filters, strides):\n",
        "        y = DepthwiseConv2D((3,3), strides=strides, padding='same')(block_input) # aplica una convolución 3x3 a cada canal por separado\n",
        "        y = BatchNormalization()(y); y = ReLU()(y)\n",
        "        y = Conv2D(filters, (1,1), padding='same')(y) # combina los canales resultantes, aprendiendo cómo mezclarlos (pointwise)\n",
        "        y = BatchNormalization()(y); y = ReLU()(y)\n",
        "        return y\n",
        "    x = dw_pw(x, 32, strides=2)   # ~(21x21x32)\n",
        "    x = dw_pw(x, 32, strides=2)   # ~(11x11x32)\n",
        "    x = dw_pw(x, 64, strides=1)   # ~(11x11x64)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dense(num_actions, activation='linear', dtype='float32')(x)\n",
        "    return Model(inputs=x_in, outputs=x)\n",
        "\n",
        "# Prueba rápida\n",
        "#model = create_mobile_like_dqn((4, 84, 84), num_actions=6)\n",
        "#model.compile(optimizer='adam', loss='mse')\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qTT0b7TqPEAh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\anaconda3\\envs\\gym4\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        }
      ],
      "source": [
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "num_actions = nb_actions\n",
        "\n",
        "model_like_dqn = create_mobile_like_dqn(input_shape, num_actions)\n",
        "#model_like_dqn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-2Yc-gUyQZii",
        "outputId": "5f50df31-02e4-4ea8-87b9-72f485e47619"
      },
      "outputs": [],
      "source": [
        "# plot_model(model_like_dqn,show_shapes=True, dpi=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "## **2. Implementación de la solución DQN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_steps = 500000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OVlMUMJWQOkH"
      },
      "outputs": [],
      "source": [
        "# memoria\n",
        "memory = SequentialMemory(limit=nb_steps, window_length=WINDOW_LENGTH)\n",
        "\n",
        "# Procesador\n",
        "processor = AtariProcessor()\n",
        "\n",
        "# Polícia de exploración\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr = 'eps', value_max = 1.0, value_min = 0.1, value_test = 0.05, nb_steps = nb_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e1qcc4VKQObW"
      },
      "outputs": [],
      "source": [
        "# Agente\n",
        "dqn_like_dqn = DQNAgent(\n",
        "    model = model_like_dqn,\n",
        "    nb_actions = num_actions,\n",
        "    policy = policy,\n",
        "    memory = memory,\n",
        "    processor = processor,\n",
        "    nb_steps_warmup = 50000, # indica cuántos pasos (interacciones con el entorno) el agente debe ejecutar antes de comenzar a entrenar el modelo\n",
        "    gamma = GAMMA,\n",
        "    target_model_update = TARGET_MODEL_UPDATE,\n",
        "    train_interval = TRAIN_INTERVAL,\n",
        "    delta_clip = DELTA_CLIP\n",
        ")\n",
        "\n",
        "optimizer = Adam(learning_rate = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UoJ5a-IwQ7Wv"
      },
      "outputs": [],
      "source": [
        "# Compilar el agente\n",
        "dqn_like_dqn.compile(optimizer = optimizer, metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "69t13LtCRqL3"
      },
      "outputs": [],
      "source": [
        "# Definimos un Callbak para poder guardar los mejores pesos cada 20 iteraciones\n",
        "class SaveBestWeights(Callback):\n",
        "    def __init__(self, filepath, interval=20, overwrite=True):\n",
        "        super(SaveBestWeights, self).__init__()\n",
        "        self.filepath = filepath\n",
        "        self.interval = interval\n",
        "        self.best_reward = -float('inf')\n",
        "        self.overwrite = overwrite\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        if episode % self.interval == 0:\n",
        "            current_reward = logs.get('episode_reward')\n",
        "            if current_reward > self.best_reward:\n",
        "                self.best_reward = current_reward\n",
        "                if self.overwrite or not os.path.exists(self.filepath):\n",
        "                    self.model.save_weights(self.filepath, overwrite=True)\n",
        "                    print(f'\\n ##### Pesos guardados en {self.filepath} recompensa: {self.best_reward}')\n",
        "                else:\n",
        "                    print(f'\\n ##### Warning: No se sobrescribió {self.filepath}, ya existe.')\n",
        "\n",
        "\n",
        "# Definimos un Callbak para poder guardar los ultimos pesos cada 10 iteraciones, sin importar si mejoró el resultado del modelo\n",
        "class SaveLastWeights(Callback):\n",
        "    def __init__(self, filepath, interval=10):\n",
        "        super(SaveLastWeights, self).__init__()\n",
        "        self.filepath = filepath\n",
        "        self.interval = interval\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        if episode % self.interval == 0:\n",
        "            self.model.save_weights(self.filepath, overwrite=True)\n",
        "            print(f'\\n ##### Pesos guardados en {self.filepath} al final del episodio {episode}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # path = 'POC4'\n",
        "\n",
        "# # # Define subdirectorios\n",
        "# # videos_path = os.path.join(path, 'videos')\n",
        "# # modelos_path = os.path.join(path, 'modelos')\n",
        "\n",
        "# # # Crear directorios si no existen\n",
        "# # os.makedirs(videos_path, exist_ok=True)\n",
        "# # os.makedirs(modelos_path, exist_ok=True)\n",
        "\n",
        "log_filename = os.path.join(modelos_path, f'dqn_{env_name}_log.json')\n",
        "\n",
        "filepath_bw = os.path.join(modelos_path, 'dqn_best_weights.h5')\n",
        "save_best_weights = SaveBestWeights(filepath=filepath_bw, interval=20, overwrite=True)\n",
        "\n",
        "filepath_lw = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "save_last_weights = SaveLastWeights(filepath=filepath_lw, interval=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "sayn-qQxQ7Qg",
        "outputId": "884dc9d8-459b-40ee-d1a7-a6f4c9f510c1"
      },
      "outputs": [],
      "source": [
        "# print(\"########--------------  modelo  ------------------###########\")\n",
        "\n",
        "# start_time = time.time()\n",
        "\n",
        "# train_scores = dqn_like_dqn.fit(env, nb_steps=nb_steps, visualize=False, verbose=1, log_interval=50000,\n",
        "#                        callbacks=[save_best_weights, save_last_weights, FileLogger(log_filename, interval=100)])\n",
        "\n",
        "# end_time = time.time()\n",
        "# training_duration = end_time - start_time\n",
        "\n",
        "# print(f\"Entrenamiento completado en {training_duration:.2f} segundos ({training_duration/60:.2f} minutos)\")\n",
        "\n",
        "# # weights_filename = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "# # dqn_like_dqn.load_weights(weights_filename)\n",
        "# # test_scores = dqn_like_dqn.test(env, nb_episodes=10, visualize=False)\n",
        "\n",
        "# # episode_rewards = test_scores.history['episode_reward']\n",
        "# # print(int(np.min(episode_rewards)), \"~\", int(np.max(episode_rewards)), \"Mean\", np.mean(episode_rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# num_episodios = len(train_scores.history['episode_reward'])\n",
        "# print(f\"Número de episodios durante el entrenamiento: {num_episodios}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testeo resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# env = wrappers.Monitor(env, videos_path, force=True)\n",
        "\n",
        "# weights_filename = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "# dqn_like_dqn.load_weights(weights_filename)\n",
        "\n",
        "# test_scores = dqn_like_dqn.test(env, nb_episodes=10, visualize=False)\n",
        "\n",
        "# episode_rewards = test_scores.history['episode_reward']\n",
        "# min_reward = int(np.min(episode_rewards))\n",
        "# max_reward = int(np.max(episode_rewards))\n",
        "# mean_reward = np.mean(episode_rewards)\n",
        "    \n",
        "# print(f\"Recompensas: {min_reward} ~ {max_reward} | Media: {mean_reward:.2f}\")\n",
        "\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obs = env.reset()\n",
        "# done = False\n",
        "# total_reward = 0\n",
        "\n",
        "# while not done:\n",
        "#     action = env.action_space.sample()  # acción aleatoria\n",
        "#     obs, reward, done, _ = env.step(action)\n",
        "#     total_reward += reward\n",
        "\n",
        "# print(\"Recompensa de episodio aleatorio:\", total_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# weights_filename = os.path.join(modelos_path, 'dqn_best_weights.h5')\n",
        "# dqn_like_dqn.load_weights(weights_filename)\n",
        "\n",
        "# test_scores = dqn_like_dqn.test(env, nb_episodes=10, visualize=False)\n",
        "\n",
        "# episode_rewards = test_scores.history['episode_reward']\n",
        "# min_reward = int(np.min(episode_rewards))\n",
        "# max_reward = int(np.max(episode_rewards))\n",
        "# mean_reward = np.mean(episode_rewards)\n",
        "    \n",
        "# print(f\"Recompensas: {min_reward} ~ {max_reward} | Media: {mean_reward:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # weights_filename = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "# # dqn_like_dqn.load_weights(weights_filename)\n",
        "\n",
        "# best_mean_reward = -float('inf')\n",
        "# best_result = None\n",
        "\n",
        "# for i in range(10):\n",
        "#     print(f\"\\n--- Evaluación #{i+1} ---\")\n",
        "#     test_scores = dqn_like_dqn.test(env, nb_episodes=10, visualize=False)\n",
        "    \n",
        "#     episode_rewards = test_scores.history['episode_reward']\n",
        "#     min_reward = int(np.min(episode_rewards))\n",
        "#     max_reward = int(np.max(episode_rewards))\n",
        "#     mean_reward = np.mean(episode_rewards)\n",
        "    \n",
        "#     print(f\"Recompensas: {min_reward} ~ {max_reward} | Media: {mean_reward:.2f}\")\n",
        "    \n",
        "#     if mean_reward > best_mean_reward:\n",
        "#         best_mean_reward = mean_reward\n",
        "#         best_result = {\n",
        "#             \"index\": i + 1,\n",
        "#             \"min\": min_reward,\n",
        "#             \"max\": max_reward,\n",
        "#             \"mean\": mean_reward\n",
        "#         }\n",
        "\n",
        "# # Mostrar el mejor resultado al final\n",
        "# print(\"\\n====== Mejor Evaluación ======\")\n",
        "# print(f\"Evaluación #{best_result['index']}: {best_result['min']} ~ {best_result['max']} | Media: {best_result['mean']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluar_pesos(dqn_agent, env, weights_path, nombre_pesos, nb_tests=10, nb_episodes=10):\n",
        "    dqn_agent.load_weights(weights_path)\n",
        "    best_mean_reward = -float('inf')\n",
        "    best_result = None\n",
        "\n",
        "    for i in range(nb_tests):\n",
        "        print(f\"\\n--- Evaluación #{i+1} para {nombre_pesos} ---\")\n",
        "        test_scores = dqn_agent.test(env, nb_episodes=nb_episodes, visualize=False)\n",
        "\n",
        "        episode_rewards = test_scores.history['episode_reward']\n",
        "        min_reward = int(np.min(episode_rewards))\n",
        "        max_reward = int(np.max(episode_rewards))\n",
        "        mean_reward = np.mean(episode_rewards)\n",
        "\n",
        "        print(f\"Recompensas: {min_reward} ~ {max_reward} | Media: {mean_reward:.2f}\")\n",
        "\n",
        "        if mean_reward > best_mean_reward:\n",
        "            best_mean_reward = mean_reward\n",
        "            best_result = {\n",
        "                \"index\": i + 1,\n",
        "                \"min\": min_reward,\n",
        "                \"max\": max_reward,\n",
        "                \"mean\": mean_reward\n",
        "            }\n",
        "    return best_result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluación #1 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 976\n",
            "Episode 2: reward: 17.000, steps: 976\n",
            "Episode 3: reward: 17.000, steps: 963\n",
            "Episode 4: reward: 17.000, steps: 963\n",
            "Episode 5: reward: 17.000, steps: 962\n",
            "Episode 6: reward: 17.000, steps: 973\n",
            "Episode 7: reward: 17.000, steps: 981\n",
            "Episode 8: reward: 17.000, steps: 972\n",
            "Episode 9: reward: 17.000, steps: 969\n",
            "Episode 10: reward: 17.000, steps: 957\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #2 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 982\n",
            "Episode 2: reward: 17.000, steps: 959\n",
            "Episode 3: reward: 17.000, steps: 973\n",
            "Episode 4: reward: 17.000, steps: 977\n",
            "Episode 5: reward: 17.000, steps: 969\n",
            "Episode 6: reward: 17.000, steps: 960\n",
            "Episode 7: reward: 17.000, steps: 972\n",
            "Episode 8: reward: 17.000, steps: 972\n",
            "Episode 9: reward: 17.000, steps: 959\n",
            "Episode 10: reward: 17.000, steps: 982\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #3 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 965\n",
            "Episode 2: reward: 17.000, steps: 969\n",
            "Episode 3: reward: 17.000, steps: 980\n",
            "Episode 4: reward: 17.000, steps: 967\n",
            "Episode 5: reward: 17.000, steps: 971\n",
            "Episode 6: reward: 17.000, steps: 969\n",
            "Episode 7: reward: 17.000, steps: 962\n",
            "Episode 8: reward: 17.000, steps: 968\n",
            "Episode 9: reward: 17.000, steps: 977\n",
            "Episode 10: reward: 17.000, steps: 973\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #4 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 989\n",
            "Episode 2: reward: 17.000, steps: 955\n",
            "Episode 3: reward: 17.000, steps: 969\n",
            "Episode 4: reward: 17.000, steps: 964\n",
            "Episode 5: reward: 17.000, steps: 964\n",
            "Episode 6: reward: 17.000, steps: 972\n",
            "Episode 7: reward: 17.000, steps: 968\n",
            "Episode 8: reward: 17.000, steps: 967\n",
            "Episode 9: reward: 17.000, steps: 964\n",
            "Episode 10: reward: 17.000, steps: 967\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #5 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 977\n",
            "Episode 2: reward: 17.000, steps: 966\n",
            "Episode 3: reward: 17.000, steps: 957\n",
            "Episode 4: reward: 17.000, steps: 976\n",
            "Episode 5: reward: 17.000, steps: 980\n",
            "Episode 6: reward: 17.000, steps: 973\n",
            "Episode 7: reward: 17.000, steps: 960\n",
            "Episode 8: reward: 17.000, steps: 965\n",
            "Episode 9: reward: 17.000, steps: 981\n",
            "Episode 10: reward: 17.000, steps: 959\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #6 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 967\n",
            "Episode 2: reward: 17.000, steps: 981\n",
            "Episode 3: reward: 17.000, steps: 974\n",
            "Episode 4: reward: 17.000, steps: 970\n",
            "Episode 5: reward: 17.000, steps: 969\n",
            "Episode 6: reward: 17.000, steps: 981\n",
            "Episode 7: reward: 17.000, steps: 974\n",
            "Episode 8: reward: 17.000, steps: 974\n",
            "Episode 9: reward: 17.000, steps: 965\n",
            "Episode 10: reward: 17.000, steps: 958\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #7 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 976\n",
            "Episode 2: reward: 17.000, steps: 970\n",
            "Episode 3: reward: 17.000, steps: 949\n",
            "Episode 4: reward: 17.000, steps: 962\n",
            "Episode 5: reward: 17.000, steps: 968\n",
            "Episode 6: reward: 17.000, steps: 973\n",
            "Episode 7: reward: 17.000, steps: 960\n",
            "Episode 8: reward: 17.000, steps: 969\n",
            "Episode 9: reward: 17.000, steps: 970\n",
            "Episode 10: reward: 17.000, steps: 974\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #8 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 962\n",
            "Episode 2: reward: 17.000, steps: 956\n",
            "Episode 3: reward: 17.000, steps: 975\n",
            "Episode 4: reward: 17.000, steps: 973\n",
            "Episode 5: reward: 17.000, steps: 957\n",
            "Episode 6: reward: 17.000, steps: 976\n",
            "Episode 7: reward: 17.000, steps: 965\n",
            "Episode 8: reward: 17.000, steps: 963\n",
            "Episode 9: reward: 17.000, steps: 962\n",
            "Episode 10: reward: 17.000, steps: 964\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #9 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 958\n",
            "Episode 2: reward: 17.000, steps: 966\n",
            "Episode 3: reward: 17.000, steps: 970\n",
            "Episode 4: reward: 17.000, steps: 981\n",
            "Episode 5: reward: 17.000, steps: 977\n",
            "Episode 6: reward: 17.000, steps: 957\n",
            "Episode 7: reward: 17.000, steps: 954\n",
            "Episode 8: reward: 17.000, steps: 955\n",
            "Episode 9: reward: 17.000, steps: 961\n",
            "Episode 10: reward: 17.000, steps: 969\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #10 para last_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 17.000, steps: 959\n",
            "Episode 2: reward: 17.000, steps: 971\n",
            "Episode 3: reward: 17.000, steps: 973\n",
            "Episode 4: reward: 17.000, steps: 966\n",
            "Episode 5: reward: 17.000, steps: 962\n",
            "Episode 6: reward: 17.000, steps: 964\n",
            "Episode 7: reward: 17.000, steps: 964\n",
            "Episode 8: reward: 17.000, steps: 969\n",
            "Episode 9: reward: 17.000, steps: 961\n",
            "Episode 10: reward: 17.000, steps: 972\n",
            "Recompensas: 17 ~ 17 | Media: 17.00\n",
            "\n",
            "--- Evaluación #1 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 924\n",
            "Episode 2: reward: 0.000, steps: 929\n",
            "Episode 3: reward: 0.000, steps: 907\n",
            "Episode 4: reward: 0.000, steps: 925\n",
            "Episode 5: reward: 0.000, steps: 922\n",
            "Episode 6: reward: 0.000, steps: 937\n",
            "Episode 7: reward: 0.000, steps: 919\n",
            "Episode 8: reward: 0.000, steps: 924\n",
            "Episode 9: reward: 0.000, steps: 926\n",
            "Episode 10: reward: 0.000, steps: 925\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #2 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 926\n",
            "Episode 2: reward: 0.000, steps: 934\n",
            "Episode 3: reward: 0.000, steps: 907\n",
            "Episode 4: reward: 0.000, steps: 922\n",
            "Episode 5: reward: 0.000, steps: 933\n",
            "Episode 6: reward: 0.000, steps: 925\n",
            "Episode 7: reward: 0.000, steps: 924\n",
            "Episode 8: reward: 0.000, steps: 913\n",
            "Episode 9: reward: 0.000, steps: 938\n",
            "Episode 10: reward: 0.000, steps: 910\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #3 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 934\n",
            "Episode 2: reward: 0.000, steps: 928\n",
            "Episode 3: reward: 0.000, steps: 916\n",
            "Episode 4: reward: 0.000, steps: 920\n",
            "Episode 5: reward: 0.000, steps: 922\n",
            "Episode 6: reward: 0.000, steps: 933\n",
            "Episode 7: reward: 0.000, steps: 914\n",
            "Episode 8: reward: 0.000, steps: 922\n",
            "Episode 9: reward: 0.000, steps: 920\n",
            "Episode 10: reward: 0.000, steps: 922\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #4 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 922\n",
            "Episode 2: reward: 0.000, steps: 923\n",
            "Episode 3: reward: 0.000, steps: 929\n",
            "Episode 4: reward: 0.000, steps: 908\n",
            "Episode 5: reward: 0.000, steps: 936\n",
            "Episode 6: reward: 0.000, steps: 915\n",
            "Episode 7: reward: 0.000, steps: 926\n",
            "Episode 8: reward: 0.000, steps: 929\n",
            "Episode 9: reward: 0.000, steps: 920\n",
            "Episode 10: reward: 0.000, steps: 917\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #5 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 922\n",
            "Episode 2: reward: 0.000, steps: 915\n",
            "Episode 3: reward: 0.000, steps: 931\n",
            "Episode 4: reward: 0.000, steps: 927\n",
            "Episode 5: reward: 0.000, steps: 927\n",
            "Episode 6: reward: 0.000, steps: 912\n",
            "Episode 7: reward: 0.000, steps: 917\n",
            "Episode 8: reward: 0.000, steps: 925\n",
            "Episode 9: reward: 0.000, steps: 921\n",
            "Episode 10: reward: 0.000, steps: 926\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #6 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 922\n",
            "Episode 2: reward: 0.000, steps: 920\n",
            "Episode 3: reward: 0.000, steps: 914\n",
            "Episode 4: reward: 0.000, steps: 944\n",
            "Episode 5: reward: 0.000, steps: 929\n",
            "Episode 6: reward: 0.000, steps: 919\n",
            "Episode 7: reward: 0.000, steps: 935\n",
            "Episode 8: reward: 0.000, steps: 937\n",
            "Episode 9: reward: 0.000, steps: 931\n",
            "Episode 10: reward: 0.000, steps: 923\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #7 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 929\n",
            "Episode 2: reward: 0.000, steps: 927\n",
            "Episode 3: reward: 0.000, steps: 929\n",
            "Episode 4: reward: 0.000, steps: 915\n",
            "Episode 5: reward: 0.000, steps: 936\n",
            "Episode 6: reward: 0.000, steps: 924\n",
            "Episode 7: reward: 0.000, steps: 919\n",
            "Episode 8: reward: 0.000, steps: 931\n",
            "Episode 9: reward: 0.000, steps: 916\n",
            "Episode 10: reward: 0.000, steps: 920\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #8 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 912\n",
            "Episode 2: reward: 0.000, steps: 917\n",
            "Episode 3: reward: 0.000, steps: 935\n",
            "Episode 4: reward: 0.000, steps: 924\n",
            "Episode 5: reward: 0.000, steps: 937\n",
            "Episode 6: reward: 0.000, steps: 925\n",
            "Episode 7: reward: 0.000, steps: 930\n",
            "Episode 8: reward: 0.000, steps: 911\n",
            "Episode 9: reward: 0.000, steps: 916\n",
            "Episode 10: reward: 0.000, steps: 911\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #9 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 921\n",
            "Episode 2: reward: 0.000, steps: 924\n",
            "Episode 3: reward: 0.000, steps: 903\n",
            "Episode 4: reward: 0.000, steps: 907\n",
            "Episode 5: reward: 0.000, steps: 909\n",
            "Episode 6: reward: 0.000, steps: 922\n",
            "Episode 7: reward: 0.000, steps: 934\n",
            "Episode 8: reward: 0.000, steps: 925\n",
            "Episode 9: reward: 0.000, steps: 921\n",
            "Episode 10: reward: 0.000, steps: 919\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "--- Evaluación #10 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 926\n",
            "Episode 2: reward: 0.000, steps: 933\n",
            "Episode 3: reward: 0.000, steps: 926\n",
            "Episode 4: reward: 0.000, steps: 916\n",
            "Episode 5: reward: 0.000, steps: 935\n",
            "Episode 6: reward: 0.000, steps: 934\n",
            "Episode 7: reward: 0.000, steps: 930\n",
            "Episode 8: reward: 0.000, steps: 914\n",
            "Episode 9: reward: 0.000, steps: 930\n",
            "Episode 10: reward: 0.000, steps: 923\n",
            "Recompensas: 0 ~ 0 | Media: 0.00\n",
            "\n",
            "====== Resultados finales ======\n",
            "Mejor resultado para last_weights: Evaluación #1: 17 ~ 17 | Media: 17.00\n",
            "Mejor resultado para best_weights: Evaluación #1: 0 ~ 0 | Media: 0.00\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejecutar evaluación para last_weights\n",
        "weights_last = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "best_last = evaluar_pesos(dqn_like_dqn, env, weights_last, 'last_weights')\n",
        "\n",
        "# Ejecutar evaluación para best_weights\n",
        "weights_best = os.path.join(modelos_path, 'dqn_best_weights.h5')\n",
        "best_best = evaluar_pesos(dqn_like_dqn, env, weights_best, 'best_weights')\n",
        "\n",
        "print(\"\\n====== Resultados finales ======\")\n",
        "print(f\"Mejor resultado para last_weights: Evaluación #{best_last['index']}: {best_last['min']} ~ {best_last['max']} | Media: {best_last['mean']:.2f}\")\n",
        "print(f\"Mejor resultado para best_weights: Evaluación #{best_best['index']}: {best_best['min']} ~ {best_best['max']} | Media: {best_best['mean']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **retrain desde el last_weights del experimento POC4 iters 100.000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = 'Experimento_9'\n",
        "\n",
        "# Define subdirectorios\n",
        "videos_path = os.path.join(path, 'videos')\n",
        "modelos_path = os.path.join(path, 'modelos')\n",
        "\n",
        "# Crear directorios si no existen\n",
        "# os.makedirs(videos_path, exist_ok = True)\n",
        "# os.makedirs(modelos_path, exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_filename = os.path.join(modelos_path, f'dqn_{env_name}_log.json')\n",
        "\n",
        "filepath_bw = os.path.join(modelos_path, 'dqn_best_weights.h5')\n",
        "save_best_weights = SaveBestWeights(filepath=filepath_bw, interval=20, overwrite=True)\n",
        "\n",
        "filepath_lw = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "save_last_weights = SaveLastWeights(filepath=filepath_lw, interval=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga pesos\n",
        "dqn_like_dqn.load_weights('./POCS_iters_100000/POC4/modelos/dqn_last_weights.h5')\n",
        "\n",
        "lr_new = 0.0001\n",
        "\n",
        "# LR fino\n",
        "dqn_like_dqn.compile(Adam(learning_rate=lr_new), metrics=['mae'])\n",
        "\n",
        "# Política de exploración moderada\n",
        "dqn_like_dqn.policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.5, value_min=0.05, value_test=0.05, nb_steps=nb_steps)\n",
        "\n",
        "save_best_weights = SaveBestWeights(filepath=filepath_bw, interval=5, overwrite=True)\n",
        "save_last_weights = SaveLastWeights(filepath=filepath_lw, interval=10)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 500000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anaconda3\\envs\\gym4\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   771/100000 [..............................] - ETA: 13:03 - reward: 0.0259\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_best_weights.h5 recompensa: 20.0\n",
            "\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 0\n",
            "  7148/100000 [=>............................] - ETA: 11:41 - reward: 0.0189\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 10\n",
            " 13792/100000 [===>..........................] - ETA: 10:02 - reward: 0.0191\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 20\n",
            " 21520/100000 [=====>........................] - ETA: 8:53 - reward: 0.0186\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 30\n",
            " 25983/100000 [======>.......................] - ETA: 8:15 - reward: 0.0186\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_best_weights.h5 recompensa: 23.0\n",
            " 29843/100000 [=======>......................] - ETA: 7:47 - reward: 0.0186\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 40\n",
            " 36939/100000 [==========>...................] - ETA: 7:00 - reward: 0.0184\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 50\n",
            " 45348/100000 [============>.................] - ETA: 6:08 - reward: 0.0182\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 60\n",
            " 52749/100000 [==============>...............] - ETA: 5:45 - reward: 0.0179\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 70\n",
            " 61459/100000 [=================>............] - ETA: 5:13 - reward: 0.0177\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 80\n",
            " 69775/100000 [===================>..........] - ETA: 4:26 - reward: 0.0179\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 90\n",
            " 76375/100000 [=====================>........] - ETA: 3:39 - reward: 0.0181\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 100\n",
            " 83953/100000 [========================>.....] - ETA: 2:35 - reward: 0.0182\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_best_weights.h5 recompensa: 24.0\n",
            "\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 110\n",
            " 90092/100000 [==========================>...] - ETA: 1:39 - reward: 0.0184\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 120\n",
            " 98761/100000 [============================>.] - ETA: 12s - reward: 0.0182\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 130\n",
            "100000/100000 [==============================] - 1058s 10ms/step - reward: 0.0182\n",
            "132 episodes - episode_reward: 13.750 [3.000, 28.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.056 - mean_eps: 0.432 - ale.lives: 1.969\n",
            "\n",
            "Interval 2 (100000 steps performed)\n",
            "  6345/100000 [>.............................] - ETA: 18:00 - reward: 0.0188\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 140\n",
            " 14368/100000 [===>..........................] - ETA: 16:21 - reward: 0.0173\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 150\n",
            " 22866/100000 [=====>........................] - ETA: 14:44 - reward: 0.0176\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 160\n",
            " 30625/100000 [========>.....................] - ETA: 13:21 - reward: 0.0177\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 170\n",
            " 40729/100000 [===========>..................] - ETA: 11:24 - reward: 0.0175\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 180\n",
            " 50467/100000 [==============>...............] - ETA: 9:32 - reward: 0.0171\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_best_weights.h5 recompensa: 25.0\n",
            "\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 190\n",
            " 57801/100000 [================>.............] - ETA: 8:04 - reward: 0.0172\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 200\n",
            " 66844/100000 [===================>..........] - ETA: 6:19 - reward: 0.0173\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 210\n",
            " 73569/100000 [=====================>........] - ETA: 5:03 - reward: 0.0177\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 220\n",
            " 82737/100000 [=======================>......] - ETA: 3:18 - reward: 0.0173\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 230\n",
            " 92043/100000 [==========================>...] - ETA: 1:31 - reward: 0.0174\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 240\n",
            "100000/100000 [==============================] - 1152s 12ms/step - reward: 0.0173\n",
            "118 episodes - episode_reward: 14.551 [2.000, 29.000] - loss: 0.009 - mae: 0.113 - mean_q: 0.145 - mean_eps: 0.365 - ale.lives: 1.995\n",
            "\n",
            "Interval 3 (200000 steps performed)\n",
            "   137/100000 [..............................] - ETA: 20:28 - reward: 0.0219\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 250\n",
            "  9953/100000 [=>............................] - ETA: 17:13 - reward: 0.0193\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 260\n",
            " 17977/100000 [====>.........................] - ETA: 15:40 - reward: 0.0185\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 270\n",
            " 27240/100000 [=======>......................] - ETA: 13:55 - reward: 0.0178\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 280\n",
            " 36977/100000 [==========>...................] - ETA: 12:03 - reward: 0.0181\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 290\n",
            " 44307/100000 [============>.................] - ETA: 10:39 - reward: 0.0183\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 300\n",
            " 53617/100000 [===============>..............] - ETA: 8:53 - reward: 0.0181\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 310\n",
            " 61927/100000 [=================>............] - ETA: 7:18 - reward: 0.0180\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 320\n",
            " 72948/100000 [====================>.........] - ETA: 5:11 - reward: 0.0175\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 330\n",
            " 82180/100000 [=======================>......] - ETA: 3:25 - reward: 0.0174\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_best_weights.h5 recompensa: 26.0\n",
            "\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 340\n",
            " 91097/100000 [==========================>...] - ETA: 1:42 - reward: 0.0174\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 350\n",
            "100000/100000 [==============================] - 1157s 12ms/step - reward: 0.0172\n",
            "109 episodes - episode_reward: 15.780 [6.000, 29.000] - loss: 0.009 - mae: 0.207 - mean_q: 0.260 - mean_eps: 0.275 - ale.lives: 1.932\n",
            "\n",
            "Interval 4 (300000 steps performed)\n",
            "  1342/100000 [..............................] - ETA: 18:11 - reward: 0.0119\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 360\n",
            "  9860/100000 [=>............................] - ETA: 17:30 - reward: 0.0166\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 370\n",
            " 20089/100000 [=====>........................] - ETA: 15:30 - reward: 0.0160\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 380\n",
            " 29674/100000 [=======>......................] - ETA: 13:42 - reward: 0.0162\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 390\n",
            " 38329/100000 [==========>...................] - ETA: 11:59 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 400\n",
            " 47233/100000 [=============>................] - ETA: 10:17 - reward: 0.0166\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 410\n",
            " 55753/100000 [===============>..............] - ETA: 8:43 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 420\n",
            " 65593/100000 [==================>...........] - ETA: 6:49 - reward: 0.0167\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 430\n",
            " 76561/100000 [=====================>........] - ETA: 4:41 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 440\n",
            " 85825/100000 [========================>.....] - ETA: 2:51 - reward: 0.0167\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 450\n",
            " 94753/100000 [===========================>..] - ETA: 1:03 - reward: 0.0169\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 460\n",
            "100000/100000 [==============================] - 1224s 12ms/step - reward: 0.0169\n",
            "107 episodes - episode_reward: 15.794 [8.000, 22.000] - loss: 0.008 - mae: 0.294 - mean_q: 0.364 - mean_eps: 0.185 - ale.lives: 1.959\n",
            "\n",
            "Interval 5 (400000 steps performed)\n",
            "  4529/100000 [>.............................] - ETA: 21:20 - reward: 0.0141\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 470\n",
            " 14337/100000 [===>..........................] - ETA: 18:55 - reward: 0.0156\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 480\n",
            " 25540/100000 [======>.......................] - ETA: 16:27 - reward: 0.0157\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 490\n",
            " 36273/100000 [=========>....................] - ETA: 15:37 - reward: 0.0159\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 500\n",
            " 44904/100000 [============>.................] - ETA: 13:38 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 510\n",
            " 55233/100000 [===============>..............] - ETA: 11:28 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 520\n",
            " 64209/100000 [==================>...........] - ETA: 9:38 - reward: 0.0169\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 530\n",
            " 74104/100000 [=====================>........] - ETA: 7:07 - reward: 0.0168\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 540\n",
            " 83585/100000 [========================>.....] - ETA: 4:37 - reward: 0.0170\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 550\n",
            " 93015/100000 [==========================>...] - ETA: 1:58 - reward: 0.0170\n",
            " ##### Pesos guardados en Experimento_9\\modelos\\dqn_last_weights.h5 al final del episodio 560\n",
            "100000/100000 [==============================] - 1723s 17ms/step - reward: 0.0174\n",
            "done, took 6319.476 seconds\n"
          ]
        }
      ],
      "source": [
        "train_scores_new = dqn_like_dqn.fit(env, nb_steps=nb_steps, visualize=False, verbose=1, log_interval=100000,\n",
        "                                    callbacks=[save_best_weights, save_last_weights, FileLogger(log_filename, interval=100)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 18.000, steps: 722\n",
            "Episode 2: reward: 18.000, steps: 724\n",
            "Episode 3: reward: 18.000, steps: 717\n",
            "Episode 4: reward: 18.000, steps: 717\n",
            "Episode 5: reward: 18.000, steps: 709\n",
            "Episode 6: reward: 18.000, steps: 720\n",
            "Episode 7: reward: 18.000, steps: 711\n",
            "Episode 8: reward: 18.000, steps: 722\n",
            "Episode 9: reward: 18.000, steps: 729\n",
            "Episode 10: reward: 18.000, steps: 718\n",
            "Recompensas: 18 ~ 18 | Media: 18.00\n"
          ]
        }
      ],
      "source": [
        "env = wrappers.Monitor(env, videos_path, force=True)\n",
        "\n",
        "weights_filename = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "dqn_like_dqn.load_weights(weights_filename)\n",
        "\n",
        "test_scores = dqn_like_dqn.test(env, nb_episodes=10, visualize=False)\n",
        "\n",
        "episode_rewards = test_scores.history['episode_reward']\n",
        "min_reward = int(np.min(episode_rewards))\n",
        "max_reward = int(np.max(episode_rewards))\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "    \n",
        "print(f\"Recompensas: {min_reward} ~ {max_reward} | Media: {mean_reward:.2f}\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluación #1 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 18.000, steps: 720\n",
            "Episode 2: reward: 18.000, steps: 724\n",
            "Episode 3: reward: 18.000, steps: 728\n",
            "Episode 4: reward: 18.000, steps: 719\n",
            "Episode 5: reward: 18.000, steps: 715\n",
            "Episode 6: reward: 18.000, steps: 722\n",
            "Episode 7: reward: 18.000, steps: 725\n",
            "Episode 8: reward: 18.000, steps: 724\n",
            "Episode 9: reward: 18.000, steps: 716\n",
            "Episode 10: reward: 18.000, steps: 727\n",
            "Recompensas: 18 ~ 18 | Media: 18.00\n",
            "\n",
            "--- Evaluación #2 para best_weights ---\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 18.000, steps: 722\n",
            "Episode 2: reward: 18.000, steps: 713\n",
            "Episode 3: reward: 18.000, steps: 712\n",
            "Episode 4: reward: 18.000, steps: 720\n",
            "Episode 5: reward: 18.000, steps: 718\n",
            "Episode 6: reward: 18.000, steps: 721\n",
            "Episode 7: reward: 18.000, steps: 698\n",
            "Episode 8: reward: 18.000, steps: 716\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ejecutar evaluación para last_weights\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# weights_last = os.path.join(modelos_path, 'dqn_last_weights.h5')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# best_last = evaluar_pesos(dqn_like_dqn, env, weights_last, 'last_weights')\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Ejecutar evaluación para best_weights\u001b[39;00m\n\u001b[0;32m      6\u001b[0m weights_best \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(modelos_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdqn_best_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m best_best \u001b[38;5;241m=\u001b[39m \u001b[43mevaluar_pesos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn_like_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_best\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m====== Resultados finales ======\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(f\"Mejor resultado para last_weights: Evaluación #{best_last['index']}: {best_last['min']} ~ {best_last['max']} | Media: {best_last['mean']:.2f}\")\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mevaluar_pesos\u001b[1;34m(dqn_agent, env, weights_path, nombre_pesos, nb_tests, nb_episodes)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_tests):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Evaluación #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre_pesos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     test_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     episode_rewards \u001b[38;5;241m=\u001b[39m test_scores\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m     min_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmin(episode_rewards))\n",
            "File \u001b[1;32md:\\anaconda3\\envs\\gym4\\lib\\site-packages\\rl\\core.py:351\u001b[0m, in \u001b[0;36mAgent.test\u001b[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[0;32m    349\u001b[0m observation \u001b[38;5;241m=\u001b[39m deepcopy(observation)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     observation, r, d, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_action_end(action)\n\u001b[0;32m    353\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n",
            "File \u001b[1;32md:\\anaconda3\\envs\\gym4\\lib\\site-packages\\rl\\core.py:522\u001b[0m, in \u001b[0;36mProcessor.process_step\u001b[1;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation, reward, done, info):\n\u001b[0;32m    511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an entire step by applying the processor to the observation, reward, and info arguments.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    # Arguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m        The tupel (observation, reward, done, reward) with with all elements after being processed.\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_reward(reward)\n\u001b[0;32m    524\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_info(info)\n",
            "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mAtariProcessor.process_observation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m observation\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# (height, width, channel)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(observation)\n\u001b[1;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_SHAPE\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m processed_observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m processed_observation\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m INPUT_SHAPE\n",
            "File \u001b[1;32md:\\anaconda3\\envs\\gym4\\lib\\site-packages\\PIL\\Image.py:2193\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2186\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2187\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2188\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2189\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2190\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2191\u001b[0m         )\n\u001b[1;32m-> 2193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejecutar evaluación para last_weights\n",
        "weights_last = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "best_last = evaluar_pesos(dqn_like_dqn, env, weights_last, 'last_weights')\n",
        "\n",
        "# Ejecutar evaluación para best_weights\n",
        "weights_best = os.path.join(modelos_path, 'dqn_best_weights.h5')\n",
        "best_best = evaluar_pesos(dqn_like_dqn, env, weights_best, 'best_weights')\n",
        "\n",
        "print(\"\\n====== Resultados finales ======\")\n",
        "print(f\"Mejor resultado para last_weights: Evaluación #{best_last['index']}: {best_last['min']} ~ {best_last['max']} | Media: {best_last['mean']:.2f}\")\n",
        "print(f\"Mejor resultado para best_weights: Evaluación #{best_best['index']}: {best_best['min']} ~ {best_best['max']} | Media: {best_best['mean']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2º intento de mejora cambiando parametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = 'Experimento_10'\n",
        "\n",
        "# Define subdirectorios\n",
        "videos_path = os.path.join(path, 'videos')\n",
        "modelos_path = os.path.join(path, 'modelos')\n",
        "\n",
        "# Crear directorios si no existen\n",
        "os.makedirs(videos_path, exist_ok = True)\n",
        "os.makedirs(modelos_path, exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_filename = os.path.join(modelos_path, f'dqn_{env_name}_log.json')\n",
        "\n",
        "filepath_bw = os.path.join(modelos_path, 'dqn_best_weights.h5')\n",
        "save_best_weights = SaveBestWeights(filepath=filepath_bw, interval=5, overwrite=True)\n",
        "\n",
        "filepath_lw = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "save_last_weights = SaveLastWeights(filepath=filepath_lw, interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga pesos\n",
        "dqn_like_dqn.load_weights('./POCS_iters_100000/POC4/modelos/dqn_last_weights.h5')\n",
        "\n",
        "lr_new = 0.00005\n",
        "\n",
        "# LR fino\n",
        "dqn_like_dqn.compile(Adam(learning_rate=lr_new), metrics=['mae'])\n",
        "dqn_like_dqn.gamma = 0.97\n",
        "# dqn_like_dqn.train_interval = 8 para el experimento 11\n",
        "\n",
        "# Política de exploración moderada\n",
        "dqn_like_dqn.policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.25, value_min=0.01, value_test=0.05, nb_steps=nb_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 500000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "   638/100000 [..............................] - ETA: 18:56 - reward: 0.0204\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_best_weights.h5 recompensa: 13.0\n",
            "\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 0\n",
            "  4245/100000 [>.............................] - ETA: 14:36 - reward: 0.0174\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_best_weights.h5 recompensa: 16.0\n",
            "  9186/100000 [=>............................] - ETA: 12:46 - reward: 0.0157\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 10\n",
            " 18620/100000 [====>.........................] - ETA: 10:52 - reward: 0.0164\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 20\n",
            " 28968/100000 [=======>......................] - ETA: 9:21 - reward: 0.0168\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_best_weights.h5 recompensa: 20.0\n",
            "\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 30\n",
            " 35131/100000 [=========>....................] - ETA: 8:37 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_best_weights.h5 recompensa: 24.0\n",
            " 40082/100000 [===========>..................] - ETA: 7:58 - reward: 0.0164\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 40\n",
            " 50017/100000 [==============>...............] - ETA: 6:48 - reward: 0.0164\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 50\n",
            " 58858/100000 [================>.............] - ETA: 9:32 - reward: 0.0166\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 60\n",
            " 68497/100000 [===================>..........] - ETA: 9:00 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 70\n",
            " 78296/100000 [======================>.......] - ETA: 7:21 - reward: 0.0164\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 80\n",
            " 87620/100000 [=========================>....] - ETA: 4:21 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 90\n",
            " 98401/100000 [============================>.] - ETA: 33s - reward: 0.0164\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 100\n",
            "100000/100000 [==============================] - 2129s 21ms/step - reward: 0.0164\n",
            "102 episodes - episode_reward: 16.039 [6.000, 25.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.047 - mean_eps: 0.214 - ale.lives: 1.947\n",
            "\n",
            "Interval 2 (100000 steps performed)\n",
            "  9849/100000 [=>............................] - ETA: 35:04 - reward: 0.0129\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 110\n",
            " 20291/100000 [=====>........................] - ETA: 33:50 - reward: 0.0142\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 120\n",
            " 29902/100000 [=======>......................] - ETA: 32:11 - reward: 0.0148\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 130\n",
            " 39609/100000 [==========>...................] - ETA: 28:04 - reward: 0.0153\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 140\n",
            " 50342/100000 [==============>...............] - ETA: 25:00 - reward: 0.0152\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 150\n",
            " 60413/100000 [=================>............] - ETA: 20:03 - reward: 0.0152\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 160\n",
            " 70041/100000 [====================>.........] - ETA: 14:43 - reward: 0.0153\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 170\n",
            " 80145/100000 [=======================>......] - ETA: 9:15 - reward: 0.0154\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 180\n",
            " 89001/100000 [=========================>....] - ETA: 5:10 - reward: 0.0158\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 190\n",
            " 96777/100000 [============================>.] - ETA: 1:32 - reward: 0.0162\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 200\n",
            "100000/100000 [==============================] - 2887s 29ms/step - reward: 0.0162\n",
            "102 episodes - episode_reward: 15.873 [6.000, 25.000] - loss: 0.008 - mae: 0.067 - mean_q: 0.105 - mean_eps: 0.178 - ale.lives: 1.956\n",
            "\n",
            "Interval 3 (200000 steps performed)\n",
            "  6953/100000 [=>............................] - ETA: 42:43 - reward: 0.0145\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 210\n",
            " 16950/100000 [====>.........................] - ETA: 38:01 - reward: 0.0158\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 220\n",
            " 26214/100000 [======>.......................] - ETA: 33:00 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 230\n",
            " 38089/100000 [==========>...................] - ETA: 24:51 - reward: 0.0159\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 240\n",
            " 46828/100000 [=============>................] - ETA: 20:30 - reward: 0.0161\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 250\n",
            " 56527/100000 [===============>..............] - ETA: 16:30 - reward: 0.0164\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_best_weights.h5 recompensa: 27.0\n",
            "\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 260\n",
            " 66774/100000 [===================>..........] - ETA: 13:09 - reward: 0.0161\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 270\n",
            " 74527/100000 [=====================>........] - ETA: 10:11 - reward: 0.0166\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 280\n",
            " 84145/100000 [========================>.....] - ETA: 6:27 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 290\n",
            " 93065/100000 [==========================>...] - ETA: 2:52 - reward: 0.0165\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 300\n",
            "100000/100000 [==============================] - 2514s 25ms/step - reward: 0.0164\n",
            "103 episodes - episode_reward: 15.961 [8.000, 27.000] - loss: 0.009 - mae: 0.129 - mean_q: 0.188 - mean_eps: 0.130 - ale.lives: 1.945\n",
            "\n",
            "Interval 4 (300000 steps performed)\n",
            "  4294/100000 [>.............................] - ETA: 42:10 - reward: 0.0128\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 310\n",
            " 14017/100000 [===>..........................] - ETA: 38:12 - reward: 0.0156\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 320\n",
            " 22705/100000 [=====>........................] - ETA: 34:01 - reward: 0.0167\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 330\n",
            " 32377/100000 [========>.....................] - ETA: 28:57 - reward: 0.0166\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 340\n",
            " 43041/100000 [===========>..................] - ETA: 24:02 - reward: 0.0163\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 350\n",
            " 51896/100000 [==============>...............] - ETA: 20:25 - reward: 0.0163\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 360\n",
            " 60710/100000 [=================>............] - ETA: 16:41 - reward: 0.0166\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 370\n",
            " 70712/100000 [====================>.........] - ETA: 12:28 - reward: 0.0168\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 380\n",
            " 79201/100000 [======================>.......] - ETA: 8:50 - reward: 0.0171\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 390\n",
            " 90296/100000 [==========================>...] - ETA: 4:04 - reward: 0.0169\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 400\n",
            " 98120/100000 [============================>.] - ETA: 48s - reward: 0.0172\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 410\n",
            "100000/100000 [==============================] - 2574s 26ms/step - reward: 0.0173\n",
            "106 episodes - episode_reward: 16.170 [7.000, 26.000] - loss: 0.010 - mae: 0.187 - mean_q: 0.258 - mean_eps: 0.082 - ale.lives: 1.973\n",
            "\n",
            "Interval 5 (400000 steps performed)\n",
            "  5577/100000 [>.............................] - ETA: 42:56 - reward: 0.0186\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 420\n",
            " 14817/100000 [===>..........................] - ETA: 36:29 - reward: 0.0186\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 430\n",
            " 22502/100000 [=====>........................] - ETA: 33:01 - reward: 0.0201\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 440\n",
            " 29416/100000 [=======>......................] - ETA: 30:13 - reward: 0.0211\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 450\n",
            " 38921/100000 [==========>...................] - ETA: 26:02 - reward: 0.0205\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 460\n",
            " 46513/100000 [============>.................] - ETA: 23:27 - reward: 0.0210\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 470\n",
            " 54306/100000 [===============>..............] - ETA: 19:50 - reward: 0.0210\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 480\n",
            " 61603/100000 [=================>............] - ETA: 16:38 - reward: 0.0214\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 490\n",
            " 68776/100000 [===================>..........] - ETA: 13:24 - reward: 0.0216\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 500\n",
            " 75588/100000 [=====================>........] - ETA: 10:26 - reward: 0.0219\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 510\n",
            " 82442/100000 [=======================>......] - ETA: 7:30 - reward: 0.0224\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 520\n",
            " 89940/100000 [=========================>....] - ETA: 4:18 - reward: 0.0225\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 530\n",
            " 97850/100000 [============================>.] - ETA: 54s - reward: 0.0223\n",
            " ##### Pesos guardados en Experimento_10\\modelos\\dqn_last_weights.h5 al final del episodio 540\n",
            "100000/100000 [==============================] - 2580s 26ms/step - reward: 0.0219\n",
            "done, took 12690.865 seconds\n"
          ]
        }
      ],
      "source": [
        "train_scores_new = dqn_like_dqn.fit(env, nb_steps=nb_steps, visualize=False, verbose=1, log_interval=100000,\n",
        "                                    callbacks=[save_best_weights, save_last_weights, FileLogger(log_filename, interval=100)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 2.000, steps: 675\n",
            "Episode 2: reward: 2.000, steps: 628\n",
            "Episode 3: reward: 5.000, steps: 896\n",
            "Episode 4: reward: 3.000, steps: 1052\n",
            "Episode 5: reward: 3.000, steps: 1098\n",
            "Episode 6: reward: 3.000, steps: 696\n",
            "Episode 7: reward: 4.000, steps: 713\n",
            "Episode 8: reward: 2.000, steps: 693\n",
            "Episode 9: reward: 2.000, steps: 704\n",
            "Episode 10: reward: 2.000, steps: 662\n",
            "Recompensas: 2 ~ 5 | Media: 2.80\n"
          ]
        }
      ],
      "source": [
        "# env = wrappers.Monitor(env, videos_path, force=True)\n",
        "\n",
        "weights_filename = os.path.join(modelos_path, 'dqn_last_weights.h5')\n",
        "dqn_like_dqn.load_weights(weights_filename)\n",
        "\n",
        "test_scores = dqn_like_dqn.test(env, nb_episodes=10, visualize=False)\n",
        "\n",
        "episode_rewards = test_scores.history['episode_reward']\n",
        "min_reward = int(np.min(episode_rewards))\n",
        "max_reward = int(np.max(episode_rewards))\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "    \n",
        "print(f\"Recompensas: {min_reward} ~ {max_reward} | Media: {mean_reward:.2f}\")\n",
        "\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 18.000, steps: 725\n",
            "Episode 2: reward: 18.000, steps: 717\n",
            "Episode 3: reward: 14.000, steps: 561\n",
            "Episode 4: reward: 18.000, steps: 724\n",
            "Episode 5: reward: 13.000, steps: 562\n",
            "Episode 6: reward: 18.000, steps: 720\n",
            "Episode 7: reward: 18.000, steps: 710\n",
            "Episode 8: reward: 14.000, steps: 563\n",
            "Episode 9: reward: 18.000, steps: 718\n",
            "Episode 10: reward: 18.000, steps: 720\n",
            "Recompensas: 13 ~ 18 | Media: 16.70\n"
          ]
        }
      ],
      "source": [
        "# env = wrappers.Monitor(env, videos_path, force=True)\n",
        "\n",
        "weights_filename = os.path.join(modelos_path, 'dqn_best_weights.h5')\n",
        "dqn_like_dqn.load_weights(weights_filename)\n",
        "\n",
        "test_scores = dqn_like_dqn.test(env, nb_episodes=10, visualize=False)\n",
        "\n",
        "episode_rewards = test_scores.history['episode_reward']\n",
        "min_reward = int(np.min(episode_rewards))\n",
        "max_reward = int(np.max(episode_rewards))\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "    \n",
        "print(f\"Recompensas: {min_reward} ~ {max_reward} | Media: {mean_reward:.2f}\")\n",
        "\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAlu8b1Gb2b"
      },
      "source": [
        "## **3. Justificación de los parámetros seleccionados y de los resultados obtenidos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFQiicXK3sO"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (gym4 fresh)",
      "language": "python",
      "name": "gym4-fresh"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
